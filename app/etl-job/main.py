import duckdb
import logging
from datetime import datetime, timezone
import os
from dataclasses import dataclass
from google.cloud import bigquery # éœ€è¦ pip install google-cloud-bigquery

# è¨­å®š Logging
logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class ETLConfig:
    gcs_bucket: str          # ä¾†æº Bucket (GCS)
    project_id: str          # GCP Project ID
    dataset_id: str          # BigQuery Dataset
    table_id: str            # BigQuery Table
    process_date: str
    memory_limit: str = "512MB"
    threads: int = 2
    temp_dir: str = "/tmp/duckdb_spill"

class DuckDBToBigQueryPipeline:
    def __init__(self, config: ETLConfig):
        self.config = config
        
        # 1. åˆå§‹åŒ– DuckDB
        self.con = duckdb.connect(config={
            'memory_limit': config.memory_limit,
            'threads': config.threads,
            'temp_directory': config.temp_dir
        })
        
        # 2. åˆå§‹åŒ– BigQuery Client (GKE æœƒè‡ªå‹•æŠ“æ¬Šé™ï¼Œä¸ç”¨å¡ Key)
        self.bq_client = bigquery.Client(project=config.project_id)
        
        self._setup_gcs_auth()

    def _ensure_temp_dir(self):
        os.makedirs(self.config.temp_dir, exist_ok=True)

    def _setup_gcs_auth(self):
        """
        DuckDB è®€å– GCS éœ€è¦ httpfs æ“´å……ã€‚
        åœ¨ GKE å…§ï¼Œé€šå¸¸ä¸éœ€è¦é¡å¤–è¨­å®š Keyï¼Œæˆ–è€…ä½¿ç”¨ HMAC Key å…¼å®¹ S3 å”è­°ã€‚
        é€™è£¡ç¤ºç¯„æœ€ç°¡å–®çš„ï¼šè®“ DuckDB çŸ¥é“æˆ‘å€‘è¦è®€é ç«¯æª”æ¡ˆã€‚
        """
        try:
            self.con.execute("INSTALL httpfs; LOAD httpfs;")
            # è‹¥åœ¨ GKE ä¸”æœ‰ Workload Identityï¼ŒDuckDB 0.10+ å¯å˜—è©¦ç›´æ¥è®€
            # ä½†æœ€ç©©çš„æ–¹å¼æ˜¯è®“ Python ä¸‹è¼‰ -> DuckDB è®€ -> ä¸Šå‚³ï¼Œ
            # æˆ–è€…è¨­å®š GCS HMAC Key (è¦–åŒ S3)ã€‚
            # é€™è£¡å‡è¨­ç’°å¢ƒè®Šæ•¸æœ‰ GCS HMAC KEY (æœ€é€šç”¨çš„è·¨é›²åšæ³•)
            if os.getenv("GCP_ACCESS_KEY_ID"):
                self.con.execute(f"""
                    SET s3_region='auto';
                    SET s3_endpoint='storage.googleapis.com';
                    SET s3_access_key_id='{os.getenv('GCP_ACCESS_KEY_ID')}';
                    SET s3_secret_access_key='{os.getenv('GCP_SECRET_ACCESS_KEY')}';
                """)
        except Exception as e:
            logger.error(f"Failed to setup DuckDB GCS extension: {e}")
            raise

    def run(self):
        logger.info(f"ğŸš€ Starting ETL: GCS -> DuckDB -> BigQuery for date: {self.config.process_date}")
        
        # 1. å®šç¾©è·¯å¾‘
        input_path = f"s3://{self.config.gcs_bucket}/raw/{self.config.process_date}/*.jsonl" # DuckDB ç”¨ s3 protocol è®€ GCS
        local_staging_file = f"{self.config.temp_dir}/agg_data.parquet"

        # 2. Extract & Transform (DuckDB)
        # é€™è£¡æˆ‘å€‘å°‡çµæœå¯«å…¥ã€Œæœ¬åœ°æš«å­˜æª”ã€ï¼Œè€Œä¸æ˜¯ç›´æ¥å¯«å› Cloud Storage
        query = f"""
        COPY (
            SELECT 
                device_id,
                '{self.config.process_date}'::DATE AS date,
                COUNT(*) AS count,
                ROUND(AVG(value), 2) AS avg_val, -- BigQuery æ¬„ä½åé¿å…ç”¨ avg é—œéµå­—
                MIN(value) AS min_val,
                MAX(value) AS max_val,
                now() AS processed_at
            FROM read_json_auto('{input_path}', format='newline_delimited')
            WHERE value >= 0 
            GROUP BY device_id
            ORDER BY device_id ASC
        ) TO '{local_staging_file}' (FORMAT 'PARQUET', CODEC 'SNAPPY');
        """

        try:
            # Step A: DuckDB é‹ç®—ä¸¦è½åœ°
            logger.info("â³ [Step 1/2] DuckDB Processing & Staging...")
            self.con.execute(query)
            logger.info(f"âœ… Staging completed: {local_staging_file}")

            # Step B: Load to BigQuery
            logger.info("â³ [Step 2/2] Loading to BigQuery...")
            self._load_parquet_to_bq(local_staging_file)
            
        except Exception as e:
            logger.error(f"âŒ ETL Failed: {e}")
            raise
        finally:
            # æ¸…ç†æš«å­˜æª” (DevOps å¥½ç¿’æ…£)
            if os.path.exists(local_staging_file):
                os.remove(local_staging_file)

    def _load_parquet_to_bq(self, parquet_file: str):
        """
        ä½¿ç”¨ Google å®˜æ–¹ SDK å°‡ Parquet ä¸Šå‚³åˆ° BigQuery
        """
        table_ref = f"{self.config.project_id}.{self.config.dataset_id}.{self.config.table_id}"
        
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.PARQUET,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND, # æˆ–æ˜¯ WRITE_TRUNCATE è¦†è“‹
        )

        with open(parquet_file, "rb") as source_file:
            job = self.bq_client.load_table_from_file(
                source_file,
                table_ref,
                job_config=job_config
            )

        job.result()  # ç­‰å¾… Job å®Œæˆ
        
        # é©—è­‰ç­†æ•¸
        table = self.bq_client.get_table(table_ref)
        logger.info(f"âœ… Loaded {job.output_rows} rows to {table_ref}. Total rows: {table.num_rows}")

# --- Entry Point ---
if __name__ == "__main__":
    # ç’°å¢ƒè®Šæ•¸æ¨¡æ“¬ (å¯¦æˆ°ä¸­ç”± Kubernetes ConfigMap/Secret æ³¨å…¥)
    config = ETLConfig(
        gcs_bucket=os.getenv("GCS_BUCKET", "my-raw-data-bucket"),
        project_id=os.getenv("GCP_PROJECT_ID", "my-gcp-project"),
        dataset_id=os.getenv("BQ_DATASET", "data_platform"),
        table_id=os.getenv("BQ_TABLE", "device_metrics"),
        process_date=os.getenv("PROCESS_DATE", datetime.now(timezone.utc).strftime("%Y-%m-%d")),
        memory_limit=os.getenv("DUCKDB_MEMORY_LIMIT", "512MB")
    )
    
    pipeline = DuckDBToBigQueryPipeline(config)
    pipeline.run()