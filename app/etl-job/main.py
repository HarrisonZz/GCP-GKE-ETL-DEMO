import duckdb
import logging
import boto3
from botocore.exceptions import ClientError
from datetime import datetime, timezone
import os
from dataclasses import dataclass

# è¨­å®š Logging
logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

@dataclass
class ETLConfig:
    s3_bucket: str
    process_date: str
    # DevOps é—œéµç´°ç¯€ï¼šé™åˆ¶è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼Œæ¨¡æ“¬åœ¨ K8s Pod é‹ä½œçš„æƒ…å¢ƒ
    memory_limit: str = "512MB" 
    threads: int = 4

class DuckDBPipeline:
    def __init__(self, config: ETLConfig):
        self.config = config
        # åˆå§‹åŒ– DuckDB é€£ç·š (In-memory mode)
        self.con = duckdb.connect(config={'memory_limit': config.memory_limit})
        self._setup_aws_auth()

    def _setup_aws_auth(self):
        """
        è‡ªå‹•è®€å–ç’°å¢ƒè®Šæ•¸ (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
        é€™ç¬¦åˆ 12-Factor App çš„è¨­å®šåŸå‰‡
        """
        try:
            self.con.execute("INSTALL httpfs; LOAD httpfs;")
            self.con.execute("CALL load_aws_credentials();")
            # è¨­å®š S3 å€åŸŸï¼Œé¿å…è·¨å€å‚³è¼¸å»¶é²
            region = os.getenv("AWS_REGION", "ap-northeast-1")
            self.con.execute(f"SET s3_region='{region}';")
        except Exception as e:
            logger.error(f"Failed to setup AWS Auth: {e}")
            raise

    def run(self):
        """
        åŸ·è¡Œ Extract -> Transform -> Aggregate -> Load
        ä¸€æ¬¡æå®š
        """
        logger.info(f"ğŸš€ Starting DuckDB ETL for date: {self.config.process_date}")
        
        input_path = f"s3://{self.config.s3_bucket}/raw/{self.config.process_date}/*.jsonl"
        output_path = f"s3://{self.config.s3_bucket}/curated/agg-{self.config.process_date}.jsonl"

        # é€™è£¡çš„ SQL é‚è¼¯ï¼š
        # 1. read_json_auto: è‡ªå‹•æ¨æ–· Schema è®€å– S3
        # 2. WHERE: éæ¿¾è² å€¼ (Data Cleaning)
        # 3. GROUP BY: èšåˆé‹ç®—
        # 4. COPY ... TO: å¯«å› S3
        
        query = f"""
        COPY (
            SELECT 
                device_id,
                '{self.config.process_date}' AS date,
                COUNT(*) AS count,
                ROUND(AVG(value), 2) AS avg,
                MIN(value) AS min,
                MAX(value) AS max,
                now() AS processed_at
            FROM read_json_auto('{input_path}')
            WHERE value >= 0 
            GROUP BY device_id

            ORDER BY device_id ASC
        ) TO '{output_path}' (FORMAT JSON);
        """

        try:
            logger.info("â³ Executing aggregation query...")
            self.con.execute(query)
            logger.info(f"âœ… ETL Job Completed! Output saved to: {output_path}")
            
            # (Optional) å¯ä»¥åœ¨é€™è£¡åšç°¡å–®çš„é©—è­‰ï¼Œç§€ä¸€ä¸‹æˆæœ
            result_preview = self.con.execute(f"SELECT * FROM read_json_auto('{output_path}') USING SAMPLE 3 ROWS").fetchall()
            logger.info(f"ğŸ‘€ Result Preview: {result_preview}")

        except Exception as e:
            logger.error(f"âŒ ETL Failed: {e}")
            raise

# --- Entry Point ---
if __name__ == "__main__":

    today_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    config = ETLConfig(
        s3_bucket=os.getenv("S3_BUCKET", "cloud-native-etl-data-dev"),
        process_date=os.getenv("PROCESS_DATE", today_str)
    )
    
    pipeline = DuckDBPipeline(config)
    pipeline.run()